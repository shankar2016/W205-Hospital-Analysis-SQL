    > 
	    > 
	    > DROP TABLE IF EXISTS effective_care_score;
OK
	Time taken: 0.066 seconds
hive> CREATE TABLE effective_care_score AS SELECT provider_id, name, avg(score) as ec_score from effective_care_parquet
    > GROUP BY provider_id, name
    > ORDER BY ec_score DESC;
Query ID = w205_20161008180505_9e4c91e6-2c0b-4fce-921b-a8701ece0d85
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0116, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0116/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0116
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-10-08 18:06:00,222 Stage-1 map = 0%,  reduce = 0%
2016-10-08 18:06:07,556 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.2 sec
2016-10-08 18:06:14,912 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.46 sec
MapReduce Total cumulative CPU time: 6 seconds 460 msec
Ended Job = job_1475607266406_0116
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0117, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0117/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0117
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2016-10-08 18:06:22,670 Stage-2 map = 0%,  reduce = 0%
2016-10-08 18:06:28,949 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.91 sec
2016-10-08 18:06:36,264 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.26 sec
MapReduce Total cumulative CPU time: 4 seconds 260 msec
Ended Job = job_1475607266406_0117
Moving data to: hdfs://localhost:8020/user/hive/warehouse/effective_care_score
Table default.effective_care_score stats: [numFiles=1, numRows=4808, totalSize=239390, rawDataSize=234582]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 6.46 sec   HDFS Read: 56101343 HDFS Write: 295705 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 4.26 sec   HDFS Read: 300097 HDFS Write: 239480 SUCCESS
Total MapReduce CPU Time Spent: 10 seconds 720 msec
OK
Time taken: 45.262 seconds
hive> 
    > 
    > DROP TABLE IF EXISTS readmissions_score;
OK
Time taken: 0.074 seconds
hive> CREATE TABLE readmissions_score AS SELECT provider_id, name, avg(score) as re_score from readmissions_parquet
    > group by provider_id, name
    > order by re_score DESC;
Query ID = w205_20161008180606_90d0bc62-c5ff-48eb-8400-aaee83b84558
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0118, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0118/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0118
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-10-08 18:06:45,260 Stage-1 map = 0%,  reduce = 0%
2016-10-08 18:06:52,803 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.59 sec
2016-10-08 18:06:59,137 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.98 sec
MapReduce Total cumulative CPU time: 5 seconds 980 msec
Ended Job = job_1475607266406_0118
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
ok to determining the best hospital from 1). Effective_care score  and 2). Readmissions score
    > 
    > -- Work with effective care first, get he average score from all hospitals
    > DROP TABLE IF EXISTS effective_care_score;
OK
Time taken: 0.007 seconds
hive> CREATE TABLE effective_care_score AS SELECT provider_id, name, avg(score) as ec_score from effective_care_parquet
    > GROUP BY provider_id, name
    > ORDER BY ec_score DESC;
Query ID = w205_20161009162929_befbc5b9-8101-4c18-9dcf-dfe93d6be260
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0229, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0229/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0229
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-10-09 16:29:11,615 Stage-1 map = 0%,  reduce = 0%
2016-10-09 16:29:20,113 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.07 sec
2016-10-09 16:29:27,515 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 6.65 sec
MapReduce Total cumulative CPU time: 6 seconds 650 msec
Ended Job = job_1475607266406_0229
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0230, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0230/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0230
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2016-10-09 16:29:36,384 Stage-2 map = 0%,  reduce = 0%
2016-10-09 16:29:42,818 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.95 sec
2016-10-09 16:29:49,207 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.29 sec
MapReduce Total cumulative CPU time: 4 seconds 290 msec
Ended Job = job_1475607266406_0230
Moving data to: hdfs://localhost:8020/user/hive/warehouse/effective_care_score
Table default.effective_care_score stats: [numFiles=1, numRows=4808, totalSize=239390, rawDataSize=234582]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 6.65 sec   HDFS Read: 56101307 HDFS Write: 295705 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 4.29 sec   HDFS Read: 300097 HDFS Write: 239480 SUCCESS
Total MapReduce CPU Time Spent: 10 seconds 940 msec
OK
Time taken: 45.589 seconds
hive> 
    > 
    > -- Now work with readmissions, get the readmissions scores
    > DROP TABLE IF EXISTS readmissions_score;
OK
Time taken: 0.006 seconds
hive> CREATE TABLE readmissions_score AS SELECT provider_id, name, avg(score) as re_score from readmissions_parquet
    > group by provider_id, name
    > order by re_score DESC;
Query ID = w205_20161009162929_be4ca174-008e-4240-9465-af482c0a9935
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0231, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0231/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0231
Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1
2016-10-09 16:29:57,212 Stage-1 map = 0%,  reduce = 0%
2016-10-09 16:30:04,674 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 3.46 sec
2016-10-09 16:30:12,106 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 5.92 sec
MapReduce Total cumulative CPU time: 5 seconds 920 msec
Ended Job = job_1475607266406_0231
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0232, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0232/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0232
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2016-10-09 16:30:20,934 Stage-2 map = 0%,  reduce = 0%
2016-10-09 16:30:27,391 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.98 sec
2016-10-09 16:30:34,827 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.41 sec
MapReduce Total cumulative CPU time: 4 seconds 410 msec
Ended Job = job_1475607266406_0232
Moving data to: hdfs://localhost:8020/user/hive/warehouse/readmissions_score
Table default.readmissions_score stats: [numFiles=1, numRows=4785, totalSize=247002, rawDataSize=242217]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 5.92 sec   HDFS Read: 17466555 HDFS Write: 295033 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 4.41 sec   HDFS Read: 299423 HDFS Write: 247090 SUCCESS
Total MapReduce CPU Time Spent: 10 seconds 330 msec
OK
Time taken: 45.604 seconds
hive> 
    > 
    > 
    > -- based on the above twor average scores, create an overall hospital score 
    > DROP TABLE IF EXISTS overall_hospital_score;
OK
Time taken: 0.006 seconds
hive> CREATE TABLE overall_hospital_score AS
    > SELECT effective_care_score.provider_id, effective_care_score.name, (readmissions_score.re_score + effective_care_score.ec_score) AS t_score
    > FROM readmissions_score FULL OUTER JOIN effective_care_score
    > ON readmissions_score.provider_id = effective_care_score.provider_id
    > order by t_score DESC;
Query ID = w205_20161009163030_b588fe4e-a265-4d93-9067-ee80f9641276
Total jobs = 2
Launching Job 1 out of 2
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0233, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0233/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0233
Hadoop job information for Stage-1: number of mappers: 2; number of reducers: 1
2016-10-09 16:30:43,890 Stage-1 map = 0%,  reduce = 0%
2016-10-09 16:30:53,726 Stage-1 map = 50%,  reduce = 0%, Cumulative CPU 2.25 sec
2016-10-09 16:30:54,763 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 4.39 sec
2016-10-09 16:31:01,222 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 7.66 sec
MapReduce Total cumulative CPU time: 7 seconds 660 msec
Ended Job = job_1475607266406_0233
Launching Job 2 out of 2
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapreduce.job.reduces=<number>
Starting Job = job_1475607266406_0234, Tracking URL = http://ip-172-31-6-204.ec2.internal:8088/proxy/application_1475607266406_0234/
Kill Command = /usr/lib/hadoop/bin/hadoop job  -kill job_1475607266406_0234
Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
2016-10-09 16:31:10,109 Stage-2 map = 0%,  reduce = 0%
2016-10-09 16:31:17,525 Stage-2 map = 100%,  reduce = 0%, Cumulative CPU 1.95 sec
2016-10-09 16:31:23,838 Stage-2 map = 100%,  reduce = 100%, Cumulative CPU 4.19 sec
MapReduce Total cumulative CPU time: 4 seconds 190 msec
Ended Job = job_1475607266406_0234
Moving data to: hdfs://localhost:8020/user/hive/warehouse/overall_hospital_score
Table default.overall_hospital_score stats: [numFiles=1, numRows=4808, totalSize=250898, rawDataSize=246090]
MapReduce Jobs Launched: 
Stage-Stage-1: Map: 2  Reduce: 1   Cumulative CPU: 7.66 sec   HDFS Read: 498597 HDFS Write: 294693 SUCCESS
Stage-Stage-2: Map: 1  Reduce: 1   Cumulative CPU: 4.19 sec   HDFS Read: 299133 HDFS Write: 250990 SUCCESS
Total MapReduce CPU Time Spent: 11 seconds 850 msec
OK
Time taken: 48.996 seconds
hive> 
    > 
    > -- display the top 10 hospitals
    > SELECT * FROM overall_hospital_score
    > LIMIT 10;
OK
450348	FALLS COMMUNITY HOSPITAL AND CLINIC	251.28333333333333
400079	HOSP COMUNITARIO BUEN SAMARITANO	184.1
451330	MEDINA REGIONAL HOSPITAL	151.25714285714287
310002	NEWARK BETH ISRAEL MEDICAL CENTER	148.61578947368423
400032	HOSPITAL HERMANOS MELENDEZ INC	145.5327485380117
051318	REDWOOD MEMORIAL HOSPITAL	144.48333333333335
261317	MERCY HOSPITAL CASSVILLE	144.1
331316	COMMUNITY MEMORIAL HOSPITAL, INC	141.47916666666666
400013	HOSPITAL MENONITA DE CAYEY	140.675
140300	PROVIDENT HOSPITAL OF CHICAGO	139.96666666666667
Time taken: 0.043 seconds, Fetched: 10 row(s)
hive> 


